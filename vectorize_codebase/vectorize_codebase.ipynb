{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODEBASE_DIR = \"./example-codebase/pytorch-examples-main/\"\n",
    "IGNORED_DIRECTORIES = [\"node_modules\", \"public/build\"]\n",
    "IGNORED_FILES = [\"package-lock.json\", \"yarn.lock\"]\n",
    "ALLOWED_EXTENSIONS = [\".ts\", \".tsx\", \".py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_EXTENSIONS = [\n",
    "    \".png\",\n",
    "    \".jpg\",\n",
    "    \".jpeg\",\n",
    "    \".gif\",\n",
    "    \".bmp\",\n",
    "    \".svg\",\n",
    "    \".ico\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codebase loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_codebase(directory):\n",
    "    snippets = []\n",
    "    for filename in os.listdir(directory):\n",
    "        # Skip hidden files and directories\n",
    "        if filename.startswith('.'):\n",
    "            continue\n",
    "\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        if os.path.isdir(filepath):\n",
    "            # If it's a directory, recursively load its contents\n",
    "            snippets.extend(load_codebase(filepath))\n",
    "        else:\n",
    "            if any(ignored in filepath for ignored in IGNORED_DIRECTORIES):\n",
    "                continue\n",
    "            if filename in IGNORED_FILES:\n",
    "                continue\n",
    "            if not any(filepath.endswith(ext) for ext in ALLOWED_EXTENSIONS):\n",
    "                continue\n",
    "\n",
    "            with open(filepath, 'r') as file:\n",
    "                content = file.read().strip()\n",
    "                if content:  # Check if content is not empty\n",
    "                    snippets.append(content)\n",
    "    return snippets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor,\n",
    "                 attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(\n",
    "        ~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(snippets):\n",
    "    prefix = \"query: \"  # Assuming all code snippets are queries\n",
    "    input_texts = [prefix + snippet for snippet in snippets]\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained('thenlper/gte-base')\n",
    "    model = AutoModel.from_pretrained('thenlper/gte-base')\n",
    "\n",
    "    batch_dict = tokenizer(input_texts, max_length=512,\n",
    "                           padding=True, truncation=True, return_tensors='pt')\n",
    "    outputs = model(**batch_dict)\n",
    "    embeddings = average_pool(\n",
    "        outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "    return F.normalize(embeddings, p=2, dim=1).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "def store_vectors(embeddings, snippets):\n",
    "    persist_directory = \"./data/db/chroma\"\n",
    "\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=snippets,\n",
    "        embedding=embeddings, \n",
    "        persist_directory=persist_directory\n",
    "        )\n",
    "\n",
    "    print(vectorstore._collection.count()) # 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_k_nearest_neighbors(query_embedding, embeddings, k=5):\n",
    "    # Using cosine similarity as embeddings are normalized\n",
    "    similarities = np.dot(embeddings, query_embedding.T)\n",
    "    sorted_indices = similarities.argsort(axis=0)[-k:][::-1]\n",
    "    return sorted_indices.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
    "\n",
    "connections.connect(host='127.0.0.1', port='19530')\n",
    "\n",
    "def create_milvus_collection(collection_name, dim):\n",
    "    if utility.has_collection(collection_name):\n",
    "        utility.drop_collection(collection_name)\n",
    "    \n",
    "    fields = [\n",
    "            FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=False),\n",
    "            FieldSchema(name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, is_primary=True, auto_id=False),\n",
    "            FieldSchema(name=\"snippet\", dtype=DataType.VARCHAR, max_length=500)\n",
    "    ]\n",
    "    schema = CollectionSchema(fields=fields, description='Code search text')\n",
    "    collection = Collection(name=collection_name, schema=schema)\n",
    "    \n",
    "    index_params = {\n",
    "        'metric_type': \"L2\",\n",
    "        'index_type': \"IVF_FLAT\",\n",
    "        'params': {\"nlist\": 2048}\n",
    "    }\n",
    "    collection.create_index(field_name='title_vector', index_params=index_params)\n",
    "    return collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "PrimaryKeyException",
     "evalue": "<PrimaryKeyException: (code=1, message=Primary key type must be DataType.INT64 or DataType.VARCHAR.)>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPrimaryKeyException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m snippets \u001b[38;5;241m=\u001b[39m load_codebase(CODEBASE_DIR)\n\u001b[1;32m      2\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m generate_embeddings(snippets)\n\u001b[0;32m----> 3\u001b[0m collection \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_milvus_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCode_search\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#store_vectors(embeddings, snippets)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# example query\u001b[39;00m\n\u001b[1;32m      7\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget references to RandomForestClassifier?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[33], line 13\u001b[0m, in \u001b[0;36mcreate_milvus_collection\u001b[0;34m(collection_name, dim)\u001b[0m\n\u001b[1;32m      7\u001b[0m     utility\u001b[38;5;241m.\u001b[39mdrop_collection(collection_name)\n\u001b[1;32m      9\u001b[0m fields \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m         FieldSchema(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mDataType\u001b[38;5;241m.\u001b[39mFLOAT_VECTOR, is_primary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, auto_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m     11\u001b[0m         FieldSchema(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mDataType\u001b[38;5;241m.\u001b[39mVARCHAR, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m     12\u001b[0m ]\n\u001b[0;32m---> 13\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mCollectionSchema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCode search text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m collection \u001b[38;5;241m=\u001b[39m Collection(name\u001b[38;5;241m=\u001b[39mcollection_name, schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[1;32m     16\u001b[0m index_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetric_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIVF_FLAT\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlist\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2048\u001b[39m}\n\u001b[1;32m     20\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/pymilvus/orm/schema.py:73\u001b[0m, in \u001b[0;36mCollectionSchema.__init__\u001b[0;34m(self, fields, description, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_kwargs()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck_fields\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_fields\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/pymilvus/orm/schema.py:117\u001b[0m, in \u001b[0;36mCollectionSchema._check_fields\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partition_key_field \u001b[38;5;241m=\u001b[39m field\n\u001b[1;32m    115\u001b[0m         partition_key_field_name \u001b[38;5;241m=\u001b[39m field\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m--> 117\u001b[0m \u001b[43mvalidate_primary_key\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_primary_field\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m validate_partition_key(partition_key_field_name,\n\u001b[1;32m    119\u001b[0m                         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_partition_key_field, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_primary_field\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    121\u001b[0m auto_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/dl/lib/python3.10/site-packages/pymilvus/orm/schema.py:39\u001b[0m, in \u001b[0;36mvalidate_primary_key\u001b[0;34m(primary_field)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PrimaryKeyException(message\u001b[38;5;241m=\u001b[39mExceptionsMessage\u001b[38;5;241m.\u001b[39mPrimaryKeyNotExist)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m primary_field\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [DataType\u001b[38;5;241m.\u001b[39mINT64, DataType\u001b[38;5;241m.\u001b[39mVARCHAR]:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PrimaryKeyException(message\u001b[38;5;241m=\u001b[39mExceptionsMessage\u001b[38;5;241m.\u001b[39mPrimaryKeyType)\n",
      "\u001b[0;31mPrimaryKeyException\u001b[0m: <PrimaryKeyException: (code=1, message=Primary key type must be DataType.INT64 or DataType.VARCHAR.)>"
     ]
    }
   ],
   "source": [
    "snippets = load_codebase(CODEBASE_DIR)\n",
    "embeddings = generate_embeddings(snippets)\n",
    "collection = create_milvus_collection('Code_search', 768)\n",
    "#store_vectors(embeddings, snippets)\n",
    "\n",
    "# example query\n",
    "query = \"get references to RandomForestClassifier?\"\n",
    "query_embedding = generate_embeddings([query])\n",
    "nearest_neighbors = find_k_nearest_neighbors(query_embedding, embeddings)\n",
    "top_matches = nearest_neighbors[:2]\n",
    "print(\"Query:\", query)\n",
    "print(\"Top Matches:\")\n",
    "for index in top_matches:\n",
    "    # print the first 500 characters to illustrate the found match\n",
    "    print(f\"- Matched Code:\\n{snippets[index][:500]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01519461 -0.01412962  0.00734623 ... -0.00222553  0.01647083\n",
      "   0.00826925]\n",
      " [-0.00277823 -0.00075571 -0.00057353 ... -0.00381351  0.01342814\n",
      "  -0.00425682]\n",
      " [-0.00645242  0.00088455 -0.0103819  ...  0.0043426   0.02141117\n",
      "   0.00353106]\n",
      " ...\n",
      " [-0.00723172 -0.02022929  0.01277409 ...  0.00926638  0.01049688\n",
      "   0.02608984]\n",
      " [-0.00788832 -0.03672303 -0.010886   ...  0.022343    0.0282235\n",
      "   0.02083131]\n",
      " [ 0.00549793 -0.00149163  0.00424555 ... -0.00705904  0.00584576\n",
      "   0.00531193]]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: get references to Deep Residual Learning for Image Recognition \n",
      "Top Matches:\n",
      "- Matched Code:\n",
      "from __future__ import print_function\n",
      "import argparse, random, copy\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import torch.optim as optim\n",
      "import torchvision\n",
      "from torch.utils.data import Dataset\n",
      "from torchvision import datasets\n",
      "from torchvision import transforms as T\n",
      "from torch.optim.lr_scheduler import StepLR\n",
      "\n",
      "\n",
      "class SiameseNetwork(nn.Module):\n",
      "    \"\"\"\n",
      "        Siamese network for image similarity estimation.\n",
      "        The network is composed of two ident...\n",
      "\n",
      "- Matched Code:\n",
      "import torch\n",
      "\n",
      "\n",
      "class TransformerNet(torch.nn.Module):\n",
      "    def __init__(self):\n",
      "        super(TransformerNet, self).__init__()\n",
      "        # Initial convolution layers\n",
      "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
      "        self.in1 = torch.nn.InstanceNorm2d(32, affine=True)\n",
      "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
      "        self.in2 = torch.nn.InstanceNorm2d(64, affine=True)\n",
      "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
      "        self.in3 = torch.n...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example query\n",
    "query = \"get references to Deep Residual Learning for Image Recognition \" #siamese network?\"\n",
    "query_embedding = generate_embeddings([query])\n",
    "nearest_neighbors = find_k_nearest_neighbors(query_embedding, embeddings)\n",
    "top_matches = nearest_neighbors[:2]\n",
    "print(\"Query:\", query)\n",
    "print(\"Top Matches:\")\n",
    "for index in top_matches:\n",
    "    # print the first 500 characters to illustrate the found match\n",
    "    print(f\"- Matched Code:\\n{snippets[index][:500]}...\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
